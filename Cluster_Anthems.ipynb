{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Clustering with K-Means\n",
    "In the present notebook we will use the [k-means algorithm](https://www.datascience.com/blog/k-means-clustering), a simple and popular __*unsupervised clustering*__ algorithm, to cluster the national anthems of the world into different groups.\n",
    "\n",
    "The objective of K-means is simple: group similar data points together and discover underlying patterns. To achieve this objective, K-means looks for a fixed defined number (k) of centroids in a dataset. A centroid refers to a cluster, which is a collection of data points aggregated together because of certain similarities with each other. The ‘means’ in the K-means refers to the averaging of the data; that is, finding the centroid. And the algorithm is said to be unsupervised because we have no prior knowledge with regards to the groups or classes of our dataset, that is, we will find the underlying groups in our dataset!\n",
    "\n",
    "Below we can visualize the algorithm. The green centroids matches the closest datapoints to each one and form clusters, then each centroid moves to the center of each respective group and matches again the closest datapoint to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](Images/kmeans.gif \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "\n",
    "__1.__ Explore our collection of national anthems (corpus) <br>\n",
    "__2.__ Data Engineer the dataset to get the best perfomance from the K-means algorithm <br>\n",
    "__3.__ Run the algorithm many times, each time testing with a different number of clusters <br>\n",
    "__4.__ Use different metrics to visualize our results and find the best number of clusters (*ie. Why are a total of X clusters better than a total of Y clusters*) <br>\n",
    "__5.__ Cluster Analysis\n",
    "\n",
    "**Metrics Utilized for Determining the Best Number of K Cluters:**\n",
    "- [Elbow Method](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "- [Silhouette Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "\n",
    "# Corpus Processing\n",
    "import re\n",
    "import nltk.corpus\n",
    "from unidecode                        import unidecode\n",
    "from nltk.tokenize                    import word_tokenize\n",
    "from nltk                             import SnowballStemmer\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "from sklearn.preprocessing            import normalize\n",
    "\n",
    "# K-Means\n",
    "from sklearn import cluster\n",
    "\n",
    "# Visualization and Analysis\n",
    "import matplotlib.pyplot  as plt\n",
    "import matplotlib.cm      as cm\n",
    "import seaborn            as sns\n",
    "from sklearn.metrics                  import silhouette_samples, silhouette_score\n",
    "from wordcloud                        import WordCloud\n",
    "\n",
    "# Map Viz\n",
    "import folium\n",
    "#import branca.colormap as cm\n",
    "from branca.element import Figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Loading:\n",
    "\n",
    "We'll use pandas to read the csv file contaning the national anthem for each country and it's corresponding country code. The anthems were extracted from wikipedia and many of them contain words that use non UTF-8 characters (generaly names of places and such), so we'll read the file with the _latin1_ encoding.\n",
    "\n",
    "Then we'll extract the __Anthem__ column into a list of texts for our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>alpha-2</th>\n",
       "      <th>alpha-3</th>\n",
       "      <th>continent</th>\n",
       "      <th>anthem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albania</td>\n",
       "      <td>AL</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Around our flag we stand united, With one wish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>AM</td>\n",
       "      <td>ARM</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Our Fatherland, free, independent, That has fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austria</td>\n",
       "      <td>AT</td>\n",
       "      <td>AUT</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Land of mountains, land by the river, Land of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>AZ</td>\n",
       "      <td>AZE</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Azerbaijan, Azerbaijan! The glorious Fatherlan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Belarus</td>\n",
       "      <td>BY</td>\n",
       "      <td>BLR</td>\n",
       "      <td>Europe</td>\n",
       "      <td>We, Belarusians, are peaceful people, Wholehea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>BE</td>\n",
       "      <td>BEL</td>\n",
       "      <td>Europe</td>\n",
       "      <td>O dear Belgium, O holy land of the fathers Ã¢â...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      country alpha-2 alpha-3 continent  \\\n",
       "0     Albania      AL     ALB    Europe   \n",
       "1     Armenia      AM     ARM    Europe   \n",
       "2     Austria      AT     AUT    Europe   \n",
       "3  Azerbaijan      AZ     AZE    Europe   \n",
       "4     Belarus      BY     BLR    Europe   \n",
       "5     Belgium      BE     BEL    Europe   \n",
       "\n",
       "                                              anthem  \n",
       "0  Around our flag we stand united, With one wish...  \n",
       "1  Our Fatherland, free, independent, That has fo...  \n",
       "2  Land of mountains, land by the river, Land of ...  \n",
       "3  Azerbaijan, Azerbaijan! The glorious Fatherlan...  \n",
       "4  We, Belarusians, are peaceful people, Wholehea...  \n",
       "5  O dear Belgium, O holy land of the fathers Ã¢â...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('datasets/anthems.csv', encoding='utf-8')\n",
    "data.columns = map(str.lower, data.columns)\n",
    "\n",
    "continents = ['Europe', 'South_America', 'North_America']\n",
    "data = data.loc[data['continent'].isin(continents)]\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"O Lord, bless the nation of Hungary With your grace and bounty Extend over it your guarding arm During strife with its enemies Long torn by ill fate Bring upon it a time of relief This nation has suffered for all sins Of the past and of the future! You brought our ancestors up Over the Carpathians' holy peaks By You was won a beautiful homeland For Bendeguz's sons And wherever flow the rivers of The Tisza and the Danube ÃƒÂ\\x81rpÃƒÂ¡d our hero's \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = data['anthem'].tolist()\n",
    "corpus[18][0:447]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Processing\n",
    "\n",
    "### 1. Stop Words and Stemming\n",
    "We will do a data engineering routine with our anthems dataset so later we can make a good statistical model. In order to do so, we'll remove all words that don't contribute to the semantic meaning of the text (words that are not within the english alphabet) and keep all of the remaining words in the simplest format possible, so we can apply a function that gives weights to each word without generating any bias or outliers. To do that there are many techniques to clean up our corpus, among them we will remove the most common words ([stop words](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)) and apply [stemming](https://www.researchgate.net/figure/Stemming-process-Algorithms-of-stemming-methods-are-divided-into-three-parts-mixed_fig2_324685008), a technique that reduces a word to it's root.\n",
    "\n",
    "The methods that apply stemming and stop words removal are listed bellow. We will also define a method that removes any words with less than 2 letters or more than 21 letters to clean our corpus even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes a list of words (ie. stopwords) from a tokenized list.\n",
    "def removeWords(listOfTokens, listOfWords):\n",
    "    return [token for token in listOfTokens if token not in listOfWords]\n",
    "\n",
    "# applies stemming to a list of tokenized words\n",
    "def applyStemming(listOfTokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in listOfTokens]\n",
    "\n",
    "# removes any words composed of less than 2 or more than 21 letters\n",
    "def twoLetters(listOfTokens):\n",
    "    twoLetterWord = []\n",
    "    for token in listOfTokens:\n",
    "        if len(token) <= 2 or len(token) >= 21:\n",
    "            twoLetterWord.append(token)\n",
    "    return twoLetterWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The main corpus processing function.\n",
    "\n",
    "A section back, at the exploration of our dataset, we noticed some words containg weird characters that should be removed. By using RegEx our main processing function will remove unknown ASCII symbols, especial chars, numbers, e-mails, URLs, etc (It's a bit of a overkill, I know). It also uses the auxiliary funcitions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCorpus(corpus, language):   \n",
    "    stopwords = nltk.corpus.stopwords.words(language)\n",
    "    param_stemmer = SnowballStemmer(language)\n",
    "    countries_list = [line.rstrip('\\n') for line in open('lists/countries.txt')] # Load .txt file line by line\n",
    "    nationalities_list = [line.rstrip('\\n') for line in open('lists/nationalities.txt')] # Load .txt file line by line\n",
    "    other_words = [line.rstrip('\\n') for line in open('lists/stopwords_scrapmaker.txt')] # Load .txt file line by line\n",
    "    \n",
    "    for document in corpus:\n",
    "        index = corpus.index(document)\n",
    "        corpus[index] = corpus[index].replace(',', '')          # Removes commas\n",
    "        corpus[index] = corpus[index].casefold()                # Makes all letters lowercase\n",
    "        \n",
    "        corpus[index] = re.sub('\\W_',' ', corpus[index])        # removes specials characters and leaves only words\n",
    "        corpus[index] = re.sub(\"\\S*\\d\\S*\",\" \", corpus[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.\n",
    "        corpus[index] = re.sub(\"\\S*@\\S*\\s?\",\" \", corpus[index]) # removes emails and mentions (words with @)\n",
    "        corpus[index] = re.sub(r'http\\S+', '', corpus[index])   # removes URLs with http\n",
    "        corpus[index] = re.sub(r'www\\S+', '', corpus[index])    # removes URLs with www\n",
    "\n",
    "        listOfTokens = word_tokenize(corpus[index])\n",
    "        twoLetterWord = twoLetters(listOfTokens)\n",
    "\n",
    "        listOfTokens = removeWords(stopwords, stopwords)\n",
    "        listOfTokens = removeWords(listOfTokens, twoLetterWord)\n",
    "        listOfTokens = removeWords(listOfTokens, countries_list)\n",
    "        listOfTokens = removeWords(theList, nationalities_list)\n",
    "        listOfTokens = removeWords(listOfTokens, other_words)\n",
    "        \n",
    "        listOfTokens = applyStemming(listOfTokens, param_stemmer)\n",
    "        listOfTokens = removeWords(listOfTokens, other_words)\n",
    "\n",
    "        corpus[index]   = \" \".join(listOfTokens)\n",
    "        corpus[index] = unidecode(corpus[index])\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lord bless nation grace bounti extend guard arm strife enemi long torn ill fate bring time relief nation suffer sin past futur brought ancestor carpathian holi peak beauti homeland bendeguz son flow river tisza danub afarpafa!d hero descend root bloom plain kun ripen wheat grape field tokaj drip sweet nectar flag plant wild turk earthwork mafa!tyafa! grave armi whimper vienna proud fort sin anger gather bosom struck lightn thunder cloud plunder mongol arro'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language = 'english'\n",
    "corpus = processCorpus(corpus, language)\n",
    "corpus[18][0:460]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
